{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs if you use google colab\n",
    "#! pip install datasets transformers torch huggingface_hub accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import bitsandbytes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Log on to hugging face to pull models\n",
    "# huggingface_hub.notebook_login() # For colab or juypter\n",
    "huggingface_hub.login(os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Figure_path': 'PMC1064097_F1.jpg', 'Question': 'What is the uptake pattern in the breast? ', 'Answer': 'Focal uptake pattern', 'Choice A': ' A:Diffuse uptake pattern ', 'Choice B': ' B:Focal uptake pattern ', 'Choice C': ' C:No uptake pattern ', 'Choice D': ' D:Cannot determine from the information given ', 'Answer_label': 'B'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load dataset \n",
    "'''\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "dataset_path = os.path.join(base_dir, \"PMC-VQA\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(dataset_path, \"train.csv\"),\n",
    "    \"test\": os.path.join(dataset_path, \"test.csv\"),\n",
    "}\n",
    "images_path = os.path.join(dataset_path, \"PMC_images_unzipped\")\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "example_image = \"C:\\\\Users\\\\scott\\\\UCF_misc\\\\MedVQA\\\\PMC-VQA\\\\PMC_images_unzipped\\\\figures_0\\\\PMC1395322_F2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load LLM\n",
    "\n",
    "# Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
    "# Memory usage: ~36 GB\n",
    "\n",
    "EXAMPLE USAGE:\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Free the inputs from GPU memory\n",
    "del inputs\n",
    "torch.cuda.empty_cache()\n",
    "'''\n",
    "\n",
    "# Set up quantization config\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    #nb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "#llm_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"\n",
    "llm_path = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "# Load tokenizer\n",
    "llm_tokenizer = transformers.AutoTokenizer.from_pretrained(llm_path)\n",
    "#llm_tokenizer.requires_grad_(False)\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "llm_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    llm_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=\"auto\", \n",
    ")\n",
    "for param in llm_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "d_llm = llm_model.config.hidden_size\n",
    "print(d_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Vison model with a linear layer. \n",
    "\n",
    "Model: openai/clip-vit-large-patch14\n",
    "Memory usage: 3 GB\n",
    "\n",
    "EXAMPLE USAGE:\n",
    "\n",
    "\n",
    "'''\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, projection_dim=d_llm):\n",
    "        #Initializes the contrastive model.\n",
    "        #projection_dim (int): The dimension of the projection space for contrastive learning.\n",
    "\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        self.vision_processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.vision_model = transformers.CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "        self.vision_model.requires_grad_(False)\n",
    "\n",
    "        '''\n",
    "        # Freeze the weights of both the vision encoder and deepseek model.\n",
    "        for param in self.vision_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Determine the output feature dimension of the vision encoder.\n",
    "        # This may vary based on your specific model.\n",
    "        # For example, CLIP's vision encoder often has a config attribute like 'hidden_size'.\n",
    "        feature_dim = (\n",
    "            self.vision_encoder.config.hidden_size\n",
    "            if hasattr(self.vision_encoder, 'config') and hasattr(self.vision_encoder.config, 'hidden_size')\n",
    "            else 768  # Fallback default dimension\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        vision_model_dim = self.vision_model.config.hidden_size\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(vision_model_dim, 4*vision_model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*vision_model_dim, projection_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            #image = Image.open()\n",
    "            inputs = self.vision_processor(images=images, return_tensors=\"pt\")      ##### Preprocess the image: resize, normalize, etc.        #####\n",
    "            outputs = self.vision_model(**inputs)                           ##### Forward pass through the vision encoder              #####\n",
    "            vision_embeddings = outputs.last_hidden_state              ##### The last_hidden_state contains the vision embeddings #####\n",
    "\n",
    "        proj_features = self.mlp(vision_embeddings)\n",
    "        #proj_features = F.normalize(proj_features, p=2, dim=1) # used in contrastive learning\n",
    "\n",
    "        return proj_features\n",
    "\n",
    "\n",
    "vision_encoder = VisionEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 62,\n",
       " 'Figure_path': 'PMC8253867_Fig2_41.jpg',\n",
       " 'Caption': 'CT pulmonary angiogram reveals encasement and displacement of the left anterior descending coronary artery ( blue arrows ).',\n",
       " 'Question': ' What is the name of the artery encased and displaced in the image? ',\n",
       " 'Choice A': ' A: Right Coronary Artery ',\n",
       " 'Choice B': ' B: Left Anterior Descending Coronary Artery ',\n",
       " 'Choice C': ' C: Circumflex Coronary Artery ',\n",
       " 'Choice D': ' D: Superior Mesenteric Artery ',\n",
       " 'Answer': 'B',\n",
       " 'split': 'test'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "dataset_path = os.path.join(base_dir, \"PMC-VQA\")\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(dataset_path, \"train_2.csv\"),\n",
    "    \"test\": os.path.join(dataset_path, \"test_2.csv\"),\n",
    "}\n",
    "images_2_path = os.path.join(dataset_path, \"images_2\")\n",
    "\n",
    "dataset_2 = load_dataset(\"csv\", data_files=data_files)\n",
    "dataset_2['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaVA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LLaVA, self).__init__()\n",
    "        self.vision_tower = vision_encoder\n",
    "        self.llm = llm_model\n",
    "        self.tokenizer = llm_tokenizer\n",
    "        self.embedding_layer = llm_model.get_input_embeddings()\n",
    "        self.prompt = (\n",
    "            \"Please Describe this image. \"\n",
    "            \"Place your description and only the description with no extra commentary after 'CAPTION:' \"\n",
    "            \"Again, place your description and only the description with no extra commentary after 'CAPTION:' \"\n",
    "        )\n",
    "\n",
    "    def forward(self, image, caption):    \n",
    "        # Get the vision embeddings\n",
    "        vision_embeds = self.vision_tower(image)\n",
    "\n",
    "        text = [\n",
    "            self.prompt + \"CAPTION: \" + cap\n",
    "            for cap in caption\n",
    "        ]\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(vision_embeds.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeds = self.embedding_layer(tokens.input_ids)\n",
    "        \n",
    "        combined_embeds = torch.cat([vision_embeds, text_embeds], dim=1)\n",
    "\n",
    "        # For the attention mask, we need one mask token per \"vision token\" plus the text tokens\n",
    "        batch_size, vision_seq_len, _ = vision_embeds.shape\n",
    "\n",
    "        vision_mask = torch.ones(batch_size, vision_seq_len, dtype=torch.long, device=combined_embeds.device)\n",
    "        text_seq_len = text_embeds.shape[1]\n",
    "        combined_attention_mask = torch.cat([vision_mask, tokens.attention_mask], dim=1)\n",
    "\n",
    "        # Need to mask out the vision embeddings \n",
    "        extended_labels = torch.full(\n",
    "            (batch_size, vision_seq_len + text_seq_len),\n",
    "            -100,  # ignore_index\n",
    "            dtype=torch.long,\n",
    "            device=vision_embeds.device\n",
    "        )\n",
    "        # Copy the text tokens into the text portion\n",
    "        extended_labels[:, vision_seq_len:] = tokens.input_ids  # the text portion\n",
    "        \n",
    "        \n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=combined_attention_mask,\n",
    "            labels=extended_labels   # teacher-forcing\n",
    "        )\n",
    "        loss = outputs.loss  # cross-entropy\n",
    "\n",
    "        return loss\n",
    "        \n",
    "\n",
    "model = LLaVA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/30521], Loss: 5.7035\n",
      "Epoch [1/1], Step [2/30521], Loss: 6.0625\n",
      "Epoch [1/1], Step [3/30521], Loss: 6.4995\n",
      "Epoch [1/1], Step [4/30521], Loss: 6.0409\n",
      "Epoch [1/1], Step [5/30521], Loss: 6.3201\n",
      "Epoch [1/1], Step [6/30521], Loss: 5.5980\n",
      "Epoch [1/1], Step [7/30521], Loss: 5.6609\n",
      "Epoch [1/1], Step [8/30521], Loss: 5.6326\n",
      "Epoch [1/1], Step [9/30521], Loss: 4.6844\n",
      "Epoch [1/1], Step [10/30521], Loss: 4.6078\n",
      "Epoch [1/1], Step [11/30521], Loss: 4.6380\n",
      "Epoch [1/1], Step [12/30521], Loss: 4.2224\n",
      "Epoch [1/1], Step [13/30521], Loss: 4.8706\n",
      "Epoch [1/1], Step [14/30521], Loss: 4.0876\n",
      "Epoch [1/1], Step [15/30521], Loss: 4.1042\n",
      "Epoch [1/1], Step [16/30521], Loss: 4.4155\n",
      "Epoch [1/1], Step [17/30521], Loss: 4.0805\n",
      "Epoch [1/1], Step [18/30521], Loss: 4.0581\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataset, device, epochs, batch_size, lr, logging_steps):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            figure_paths = batch['Figure_path']  # list of image filenames\n",
    "            captions     = batch['Caption']      # list of caption strings\n",
    "\n",
    "            # --- Load and transform images ---\n",
    "            images = []\n",
    "            for fig_path in figure_paths:\n",
    "                img_full_path = os.path.join(\"./PMC-VQA/images_2/figures/\", fig_path)\n",
    "                img = Image.open(img_full_path)\n",
    "                images.append(img)\n",
    "\n",
    "            #images = torch.stack(images).to(device)\n",
    "            # Forward pass for training, returns cross-entropy loss\n",
    "            loss = model.forward(images, captions)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % logging_steps == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{step+1}/{len(train_loader)}], \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "train(model, dataset_2['train'], device, 1, 5, 0.0001, 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
